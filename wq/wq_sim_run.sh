#!/bin/bash

# this is a script designed to run and store topcoffea-analysis runs
MAX_RETRIES=75

#source /scratch365/blyons1/miniconda3/etc/profile.d/conda.sh
#conda activate os-project

#module load conda
#conda init
#source /opt/crc/c/conda/miniconda3/4.9.2/etc/profile.d/conda.sh
source ~/miniconda3/etc/profile.d/conda.sh
#source .bashrc
#conda env create -f ~tfisher4/environment.yml
conda activate gradosproj
#function dask_scheduler_start(){
#	dask-scheduler &> scheduler.log &
#}

# first argument is number of workers to wait for
# second argument is job number
function check_workers_ready(){
	COUNT=0
	sleep 1
	# NUM=$(condor_q | grep "blyons1.*running" | awk '{print $13;}')
	NUM=$(/afs/crc.nd.edu/user/c/condor/software/bin/condor_q | grep $2 | awk '{print $7;}')

	if [ "$NUM" = "_" ]; then
		NUM=0
	fi

	while [[ "$NUM" -lt "$1" && "$COUNT" -lt "$MAX_RETRIES" ]];
	do
		((COUNT=$COUNT+1))
        echo "$NUM/$1 workers ready. Going to sleep (retry $COUNT)"
		sleep 10
		NUM=$(/afs/crc.nd.edu/user/c/condor/software/bin/condor_q | grep "$2" | awk '{print $7;}') ;
		if [ "$NUM" = "_" ]; then
			NUM=0
		fi
	done

 	if [ "$NUM" -eq "$1" ]; then
 		echo "All workers ready!"
 		#return 0
 	else
 		echo "Exceeded max retries for workers "
 		#return 1
 	fi
}

function weak_benchmark(){
	# We can run several weak_benchmarks at once, since they dont interact/interfere at all, once workers are acquired and ready.
    # However, we start them one at a time (async=1 means return after workers are started and manager is running).
    # This is so we can simply use one submit file we change every time.
    # Theoretically we could use a different submit file for each experiment, but most of the speedup from parallelizing
    # these are when the benchmark itself is running, not the setup.

	benchmarkfile="wq_weak_benchmarks.py"
	benchmarkmgr="wq-weak-benchmarks-tfisher4"
    out_dir="$1"
	for workers in 1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 45 50 55 60 70 80 90 100; do
        # async=1: start benchmark in background so we can kick off others
		#run_benchmark "$benchmarkfile" "$benchmarkmgr-$(printf "%03d" "$workers")" "$workers" "$out_dir" "1" #> /dev/null # &
		run_benchmark "$benchmarkfile" "$benchmarkmgr-$(printf "%03d" "$workers")" "$workers" "$out_dir" "0" #> /dev/null # &
	done
}

function size_benchmark(){
	# We can run several weak_benchmarks at once, since they dont interact/interfere at all, once workers are acquired and ready.
    # See comment for weak_benchmark
	benchmarkfile="wq_size_benchmarks.py"
	benchmarkmgr="wq-size-benchmarks-tfisher4"
    out_dir="$1"
	for workers in 1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 45 50 55 60 70 80 90 100; do
        echo "size-$workers started"
        # async=1: start benchmark in background so we can kick off others
		run_benchmark "$benchmarkfile" "$benchmarkmgr-$(printf "%03d" "$workers")" "$workers" "$out_dir" "1" > /dev/null # &
        echo "size-$workers ended"
	done
}

function fs_benchmark(){
	# Don't want to run many fs benchmarks at once, because they all access the same file,
	# so running more benchmarks increases demand for resource outside of that generated by experiment.
	benchmarkfile="wq_fs_benchmarks.py"
	benchmarkmgr="wq-fs-benchmarks-tfisher4"
	movefile="/scratch365/tfisher4/grados-proj/bible%s.txt"
    out_dir="$1"
	for workers in 1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 45 50 55 60 70 80 90 100; do
        echo "fs-$workers started"
		cp $(printf $movefile 1) $(printf $movefile 2)
        # async=0: run benchmark in foreground so we only have one running at a time
        run_benchmark "$benchmarkfile" "$benchmarkmgr-$(printf "%03d" "$workers")" "$workers" "$out_dir" "0" > /dev/null
		rm -f $(printf $movefile 2)
        echo "fs-$workers ended"
	done
}

# this function requests and sets up condor workers
# with dask workers
# first argument is number of workers
# second is number of cpus
# third is memory, fourth is disk
# fifth is manager name
function wq_submit_workers(){
	cat > workers/condor_wq_submit_file <<EOF
universe = vanilla
executable = /scratch365/tfisher4/grados-proj/wq_worker.sh
arguments = $5
should_transfer_files = yes
when_to_transfer_output = on_exit
error = workers/${5}.worker.\$(Process).error
output = workers/${5}.worker.\$(Process).output
request_cpus = $2
request_memory = $3
request_disk = $4
+JobMaxSuspendTime = 0
queue $1
EOF
#log = workers/workers.log
#arguments = tcp://10.32.85.47:8790
#when_to_transfer_output = on_exit
#error = /scratch365/tfisher4/grados-proj/workers/${5}.worker.\$(Process).error
	JOB_NUM=$(/afs/crc.nd.edu/user/c/condor/software/bin/condor_submit workers/condor_wq_submit_file | grep -Eo "[0-9]+\." | grep -Eo "[0-9]+")
	check_workers_ready "$1" "$JOB_NUM"
	echo "$JOB_NUM"
}

function run_benchmark(){
	benchmarkfilefull="$1"
	benchmarkmgrfull="$2"
	workers="$3"
    out_dir="$4"
    async="$5"

	echo "getting workers"
	#JOB_NUM=$(condor_submit_workers --cores 4 --memory 16000 --disk 16000 --manager-name "$benchmarkmgr" "$workers" | awk '{print $NF}'| tail -1 | tr -d '.')
	JOB_NUM=$(wq_submit_workers "$workers" 4 16000 16000 "$benchmarkmgrfull")
	echo "job $JOB_NUM"
	#check_workers_ready "$workers" "$JOB_NUM"
	echo "workers ready"
	#sleep 10
	echo "starting benchmark $benchmarkmgrfull"
    if [ "$async" -eq "1" ]; then
	    python "$benchmarkfilefull" "$workers" "$out_dir" &
    else
	    python "$benchmarkfilefull" "$workers" "$out_dir"
    fi

	#python dask_task_benchmarks.py $workers
	#condor_rm "$JOB_NUM"
}

function run_all(){
    out_dir="$1"
	weak_benchmark "$out_dir"
	size_benchmark "$out_dir"
	fs_benchmark "$out_dir"
	wait # wait for all benchmarks to complete
}
